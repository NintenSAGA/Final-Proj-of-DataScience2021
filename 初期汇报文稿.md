## 数据科学基础大作业-司法大数据自动化标注与分析

谭子悦 李佳骏 邱兴驰

### 1. 选题理解、思路



#### 1.1 爬虫

爬虫部分的总体思路为：

> 1. 给定参数：搜索目标，限制条件，目标条数……
> 2. 利用自动化Web工具获取目标网页上的全部相关条目对应的URL，将URL条目存在URL_LIST中
> 3. 根据URL_LIST取回HTML文件缓存在本地文件夹
> 4. 利用HTML解析工具提取文本
> 5. 对文本进行净化与切分，输出纯文本

功能特性：

- 可以设置爬取起始页与文书数量
- 可以进行自动化爬取
- 可以分别设置：
  - 只爬取URL_LIST
  - （已有URL_LIST时）直接取回HTML并提取文本
  - （已有HTML缓存时）直接提取文本

#### 1.2 自动化标注@



#### 1.3 拓展



### 2. 实现

#### 2.1 爬虫

<img src="./resources/Crawling-map.png" alt="Crawling-map"/>

如图所示，爬虫部分可拆分为两个大模块：url_fetch和text_extract

实现流程为：

> **A. url_fetch**
>
> 1. 利用Selenium作为自动化爬取工具
> 2. Web Driver选用Microsoft Edge
>    - 可自动识别系统切换不同的Web Driver内核（仅支持macOS和Windows)
>    - 使用前需先安装Microsoft Edge
> 3. 打开网页
>    - 文书来源可选裁判文书网、北大法宝、中华人民共和国最高法院公报
>    - 出于运行稳定性考虑，优先选择北大法宝作为来源（需在南大内网使用）
> 4. 根据给定参数设置筛选条件
>    - 默认选择 普通案例、刑事一审、判决书
> 5. 获取`n`个条目链接，存储于 `./result/~url_list.txt`
>
> **B. text_extract**
>
> 1. 建立`mechanicalsoup.StatefulBrowser`对象browser
>
> 2. 将Selenium Web Driver的cookies转移给browser，防止因并发访问被网站屏蔽
>
> **a. html_file_retrieve**
>
> 3. 用browser获取`url_list.txt`中每一个条目对应的HTML文档
>
> **b. html_text_retrieve**
>
> 4. 利用Beautiful Soup提取每一个HTML文档中的文本
> 5. 提炼文本，得到最终的纯文本，储存为 `[序号].[文书名].txt`



#### 2.2 自动化标注@



#### 2.3 拓展



#### 2.4 大致界面

<img src="./resources/panel0.png" alt="Crawling-map" width="30%" /> 

<img src="./resources/panel1.png" alt="Crawling-map" width="30%" /> 

<img src="./resources/panel2.png" alt="Crawling-map" width="50%" />

界面主要可分为三个部分：选择页面、爬虫页面、批注页面

### 3. 当前进度展示

#### 3.1 爬虫



#### 3.2 自动化标注@
